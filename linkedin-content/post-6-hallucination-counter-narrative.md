# Post 6: The "Hallucination" Counter-Narrative

**Status:** Draft
**Angle:** Contrarian - reframe a common AI criticism
**Visuals:** TBD

---

"AI hallucinates too much. I can't trust it."

I hear this constantly. And honestly? It's a skill issue.

Here's what I've learned coaching product and engineering leaders on AI-assisted development:

AI doesn't hallucinate when it has clear instructions.
It hallucinates when YOU'RE unclear.

Think about it:

When you give a vague spec to an engineer, they fill in the gaps with assumptions. Sometimes wrong ones.

AI does the same thing — just faster and more confidently.

The fix isn't better AI. It's better inputs.

Here's what I do:

→ I keep a folder of markdown files with explicit instructions
→ Naming conventions. Code patterns. Testing requirements. Architecture decisions.
→ Before Claude does anything, it reads that folder first.

The result?

It executes for hours without mistakes. Builds features. Runs tests. Writes documentation.

No hallucinations. Because there's no ambiguity.

"But I shouldn't have to write all that documentation."

You're right. You shouldn't have to.

But you probably should have written it years ago — for your human engineers too.

AI just made the cost of unclear thinking impossible to ignore.
